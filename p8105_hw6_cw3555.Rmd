---
title: "p8105_hw6_cw3555"
author: "Eunice Wang"
date: '`2023-12-02`'
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Load key packages.

```{r}
library(tidyverse)
library(modelr)
library(p8105.datasets)

set.seed(1)
```

### Problem 1

#### Load and clean the raw data.

```{r}
homicide_df=
  read_csv("data/homicide-data.csv") |> 
  janitor::clean_names() |> 
  mutate(
    city_state=str_c(city,state,sep =", "),
    resolved = as.numeric(disposition == "Closed by arrest"), 
    victim_age=as.numeric(victim_age)
    )|> 
      filter(!city_state %in% c("Dallas, TX","Phoenix, AZ", "Kansas City, MO", "Tulsa, AL" )) |> 
      filter(victim_race %in% c("Black", "White")) |> 
  select(city_state,resolved, victim_age, victim_sex, victim_race)
```

#### Fit a logistic regression for Baltimore, MD and save the results for estimates

```{r}
fit_baltimore=
  homicide_df |> 
  filter(city_state=="Baltimore, MD") |> 
  glm(
    resolved ~ victim_age + victim_sex + victim_race, 
    data = _, 
    family = binomial())

baltimore_results=
fit_baltimore |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate-1.96*std.error),
    OR_CI_upper = exp(estimate+1.96*std.error)
    ) |> 
  filter(term=="victim_sexMale") |> 
  select(OR,OR_CI_lower,OR_CI_upper) 
```

The estimated proportion of unsolve homicides in Baltimore, MD is `r pull(baltimore_results,OR)`and the CI is [`r pull(baltimore_results,OR_CI_lower)`, `r pull(baltimore_results,OR_CI_upper)`]


#### Fit a logistic regression for each of the cities and save the results for estimates

```{r}
city_glm_model=
  homicide_df |> 
  nest(df = -city_state) |> 
  mutate(
    models = map(df, \(df) glm(resolved ~ victim_age + victim_sex +victim_race, family = binomial(), data = df)),
    results = map(models, broom::tidy)
  ) |>
  select(city_state, results) |> 
  unnest(results) |> 
  mutate(
    OR = exp(estimate),
    OR_CI_lower = exp(estimate-1.96*std.error),
    OR_CI_upper = exp(estimate+1.96*std.error)
    ) |> 
    filter(term=="victim_sexMale") |> 
    select(city_state,OR,OR_CI_lower,OR_CI_upper) 

city_glm_model |> 
  head() |> 
  knitr::kable(digits = 2) 
```

#### Create a plot that shows the estimates for each city and comment

```{r}
city_glm_model |>  
  ggplot(aes(x=fct_reorder(city_state, OR), y=OR, ymin=OR_CI_lower, ymax=OR_CI_upper))+
  geom_point()+
  geom_errorbar()+
  theme(axis.text.x=element_text(angle=90, vjust=0.5,hjust=1))+
  labs(
    x="City_State",
    y="Adjuested Estimated Odds Ratio" ,
    title="The estimated ORs and CIs for each city"
  )
```

The plots shows the estimated ORs and CIs for each city. The estimated odds ratio is ordered from the smallest estimated OR to the largest one. In most cities, the estimated odds ratio is less than 1. CIs in most cities are narrow and do not contain 1, which implies a significant difference in resolved rates between both sexes after adjustment.

### Problem 2

#### Load and clean the raw data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

#### Fit a linear regression
```{r}
weather_lm=
  lm(tmax ~ tmin+prcp, data=weather_df)
broom::tidy(weather_lm)
broom::glance(weather_lm)
```

#### Bootstrap for r̂ 2

```{r}
boot_results_rsq = 
  weather_df |> 
   modelr::bootstrap(n = 5000) |> 
   mutate(
    models = map(strap, \(df) lm(tmax ~ tmin+prcp, data=df)),
    results = map(models, broom::glance),
  ) |> 
  select(results) |> 
  unnest(results) 
```

Make a plot of r̂ 2

```{r}
boot_results_rsq|> 
  select(r.squared) |> 
  ggplot(aes(x=r.squared))+
  geom_density()+
  labs(
    x="Bootstrap R-squared",
    y="Frequency",
    title="Distribution of Bootstrap Estimated R-squared"
  )
```

This plot for `r-squared` measure the goodness of fit regarding a model. From the plot, we can see that most of the estimates of `r-squared` range from 0.90 to 0.94, with a peak around 0.92. The majority of the bootstrap samples has a value of 0.92 for its `r_squared`.This indicates a generally good fit of the model. The distribution is slightly skewed to the left.

Provide the 95% confidence interval for r̂ 2
```{r}
CI_r_squared=
  boot_results_rsq|> 
  summarize(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975)
  ) 
```
The CI for `r_squared` is [`r pull(CI_r_squared,ci_lower)`, `r pull(CI_r_squared,ci_upper)`]

#### Bootstrap for  log(β̂ 1∗β̂ 2

```{r}
boot_results_log_beta = 
  weather_df |> 
   modelr::bootstrap(n = 5000) |> 
   mutate(
    models = map(strap, \(df) lm(tmax ~ tmin+prcp, data=df)),
    results = map(models, broom::tidy),
  ) |> 
  select(id=.id, results) |> 
  unnest(results) |> 
  select(id, term, estimate) |> 
  pivot_wider(
    names_from=term,
    values_from=estimate) |> 
  rename(beta_1=tmin, beta_2=prcp)
```

Make a plot for log(β̂ 1∗β̂ 2)

```{r}
filtered_boot_results=
  boot_results_log_beta |> 
  filter(beta_1*beta_2>0) |> 
  mutate(log_beta_product=log(beta_1*beta_2))

filtered_boot_results|> 
  ggplot(aes(x=pull(filtered_boot_results, log_beta_product)))+
  geom_density()+
  labs(
    x="Bootstrap Log(Beat1*Beta2)",
    y="Frequency",
    title="Distribution of Bootstrap Estimated Log(Beat1*Beta2)"
  )
```

This plot for `log(beat1*beta2)` has a left-skewed distribution wuth a long tail extending towards lower values. From the plot, we can see that most of the estimates of `log(beat1*beta2)` lies in -7 to -5, with a peak around -5.5.The majority of the bootstrap samples has a value of -5.5 for its `log(beat1*beta2)`, indicating a moderate influence of the two factors.

Provide the 95% confidence interval for log(β̂ 1∗β̂ 2)

```{r}
CI_log_beta_product=
  filtered_boot_results|> 
  summarize(
    ci_lower = quantile(log_beta_product, 0.025),
    ci_upper = quantile(log_beta_product, 0.975)
  )
```

The CI for `log_beta_product` is [`r pull(CI_log_beta_product,ci_lower)`, `r pull(CI_log_beta_product,ci_upper)`]

### Problem 3

#### Load and clean the raw data.

```{r}
bw_df=
  read_csv("data/birthweight.csv") |> 
  janitor::clean_names() |> 
  mutate(
    babysex=as.factor(babysex),
    frace=as.factor(frace),
    malform=as.factor(malform),
    mrace=as.factor(mrace)
  ) 
```

Check for missing data:

```{r}
sum(is.na(bw_df))
```
There is no missing data in the dataset.

#### Propose a regression model for birthweight

```{r}
fit_bwt_1=
  bw_df |> 
  lm(bwt ~babysex+fincome+delwt+wtgain, data=_)

fit_bwt_1 |> 
  broom::tidy() |> 
  knitr::kable()
```

My regression model for birth weight use sex of baby, family monthly income, mother’s weight at delivery and mother’s weight gain during pregnancy as predictors. From the table we can see, the coefficients are all statistically significant.

#### Add predictions & residuals, make a plot of model residuals against fitted values
```{r}
bw_1=
  bw_df |> 
  add_predictions(fit_bwt_1) |> 
  add_residuals(fit_bwt_1)

bw_1 |> 
  ggplot(aes(x=pred,y=resid))+
  geom_point(alpha=0.5)+
  geom_smooth(method="lm")+
  labs(
    x="Fitted alues",
    y="Residuals",
    title="Residuals Against Fitted Values"
  )
```

From the plot, we can see that there is no clear pattern. From the smooth line, we can see there is no linear association. 

#### Compare my model to the other two models

One using length at birth and gestational age as predictors (main effects only)
```{r}
fit_bwt_2=
  bw_df |> 
  lm(bwt ~blength+gaweeks, data=_)

fit_bwt_2 |> 
  broom::tidy() |> 
  knitr::kable()
```

One using head circumference, length, sex, and all interactions (including the three-way interaction) between these

```{r}
fit_bwt_3=
  bw_df |> 
  lm(bwt ~bhead*blength*babysex, data=_)

fit_bwt_3 |> 
  broom::tidy() |> 
  knitr::kable()
```

Make a comparison

```{r}
cv_df = 
  crossv_mc(bw_df, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )
```

```{r}
cv_results =
  cv_df |> 
  mutate(
    fit_1 = map(train, \(df) lm(bwt ~babysex+fincome+delwt+wtgain, data = df)),
    fit_2 = map(train, \(df) lm(bwt ~blength+gaweeks, data=df)),
    fit_3 = map(train, \(df) lm(bwt ~bhead*blength*babysex, data=df))) |> 
  mutate(
    rmse_fit_1 = map2_dbl(fit_1, test, \(mod, df) rmse(mod, df)),
    rmse_fit_2 = map2_dbl(fit_2, test, \(mod, df) rmse(mod, df)),
    rmse_fit_3 = map2_dbl(fit_3, test, \(mod, df) rmse(mod, df))
  )
```

```{r}
cv_results |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model",
    values_to = "rmse",
    names_prefix = "rmse_"
  ) |> 
  mutate(model=fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) +
  geom_violin()
```

The violin plot provides a comparison of distribution of RMSE across different models. My model is the worst because it has the highest RMSE. The 3rd model is the best because it has the lowest RMSE. It is perhaps it takes consideration of variable interactions, which captures complex relationship and provides a better fit.
